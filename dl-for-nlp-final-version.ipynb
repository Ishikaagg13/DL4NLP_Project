{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8245500,"sourceType":"datasetVersion","datasetId":4891853},{"sourceId":8253161,"sourceType":"datasetVersion","datasetId":4897351},{"sourceId":8253166,"sourceType":"datasetVersion","datasetId":4897354},{"sourceId":8257374,"sourceType":"datasetVersion","datasetId":4900529},{"sourceId":8257380,"sourceType":"datasetVersion","datasetId":4900533},{"sourceId":8257436,"sourceType":"datasetVersion","datasetId":4900564},{"sourceId":8257438,"sourceType":"datasetVersion","datasetId":4900566},{"sourceId":8257770,"sourceType":"datasetVersion","datasetId":4900806},{"sourceId":8283789,"sourceType":"datasetVersion","datasetId":4919997},{"sourceId":8283908,"sourceType":"datasetVersion","datasetId":4920096},{"sourceId":8284123,"sourceType":"datasetVersion","datasetId":4920262},{"sourceId":8284127,"sourceType":"datasetVersion","datasetId":4920265}],"dockerImageVersionId":30699,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! nvidia-smi","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-01T19:50:28.211697Z","iopub.execute_input":"2024-05-01T19:50:28.212102Z","iopub.status.idle":"2024-05-01T19:50:29.309024Z","shell.execute_reply.started":"2024-05-01T19:50:28.212065Z","shell.execute_reply":"2024-05-01T19:50:29.307696Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Wed May  1 19:50:29 2024       \n+---------------------------------------------------------------------------------------+\n| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |\n|-----------------------------------------+----------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n|                                         |                      |               MIG M. |\n|=========================================+======================+======================|\n|   0  Tesla P100-PCIE-16GB           Off | 00000000:00:04.0 Off |                    0 |\n| N/A   36C    P0              26W / 250W |      0MiB / 16384MiB |      0%      Default |\n|                                         |                      |                  N/A |\n+-----------------------------------------+----------------------+----------------------+\n                                                                                         \n+---------------------------------------------------------------------------------------+\n| Processes:                                                                            |\n|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n|        ID   ID                                                             Usage      |\n|=======================================================================================|\n|  No running processes found                                                           |\n+---------------------------------------------------------------------------------------+\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## **Finetunning the llava model(VSFT on the llava-hf/llava-1.5-7b-hf model with 260k image and conversation pairs from the HuggingFaceH4/llava-instruct-mix-vsft dataset.) with custom dataset**","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U \"transformers>=4.39.0\"\n!pip install peft bitsandbytes\n!pip install -U \"trl>=0.8.3\"\n!pip install deep_translator","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:50:29.311737Z","iopub.execute_input":"2024-05-01T19:50:29.312795Z","iopub.status.idle":"2024-05-01T19:51:46.439188Z","shell.execute_reply.started":"2024-05-01T19:50:29.312746Z","shell.execute_reply":"2024-05-01T19:51:46.437764Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\nfrom trl import SFTTrainer\nfrom peft import LoraConfig","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:51:46.440937Z","iopub.execute_input":"2024-05-01T19:51:46.441288Z","iopub.status.idle":"2024-05-01T19:52:05.578116Z","shell.execute_reply.started":"2024-05-01T19:51:46.441255Z","shell.execute_reply":"2024-05-01T19:52:05.577231Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-01 19:51:53.974924: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-01 19:51:53.975029: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-01 19:51:54.103281: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"model_id = \"llava-hf/llava-1.5-7b-hf\"\n# model_id = \"HuggingFaceH4/vsft-llava-1.5-7b-hf\"","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:52:05.580799Z","iopub.execute_input":"2024-05-01T19:52:05.581863Z","iopub.status.idle":"2024-05-01T19:52:05.586036Z","shell.execute_reply.started":"2024-05-01T19:52:05.581822Z","shell.execute_reply":"2024-05-01T19:52:05.585003Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"quantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:52:05.587086Z","iopub.execute_input":"2024-05-01T19:52:05.587366Z","iopub.status.idle":"2024-05-01T19:52:05.620681Z","shell.execute_reply.started":"2024-05-01T19:52:05.587341Z","shell.execute_reply":"2024-05-01T19:52:05.619810Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"model = LlavaForConditionalGeneration.from_pretrained(model_id,\n                                                      quantization_config=quantization_config,\n                                                      torch_dtype=torch.float16)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:52:05.621953Z","iopub.execute_input":"2024-05-01T19:52:05.622285Z","iopub.status.idle":"2024-05-01T19:54:01.418091Z","shell.execute_reply.started":"2024-05-01T19:52:05.622258Z","shell.execute_reply":"2024-05-01T19:54:01.417143Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fbb158faabf247c098f4067ab7b7cb26"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n  warnings.warn(\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"94f5d60426fa497388933e09b04f40ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8618d01018af40198413888634ad872c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba92eab2e8804824a24ab16924d82526"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1c6dff39a4147c6981b66baadb950c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c04a01e3074845378df8a26ed1ca469a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f92f64e300af425aaf716a43b0c0e4ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b9daa68de7504b19a93c5140585e1070"}},"metadata":{}}]},{"cell_type":"code","source":"LLAVA_CHAT_TEMPLATE = \"\"\"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. {% for message in messages %}{% if message['role'] == 'user' %}USER: {% else %}ASSISTANT: {% endif %}{% for item in message['content'] %}{% if item['type'] == 'text' %}{{ item['text'] }}{% elif item['type'] == 'image' %}<image>{% endif %}{% endfor %}{% if message['role'] == 'user' %} {% else %}{{eos_token}}{% endif %}{% endfor %}\"\"\"","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:01.419514Z","iopub.execute_input":"2024-05-01T19:54:01.419940Z","iopub.status.idle":"2024-05-01T19:54:01.426779Z","shell.execute_reply.started":"2024-05-01T19:54:01.419904Z","shell.execute_reply":"2024-05-01T19:54:01.425722Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.chat_template = LLAVA_CHAT_TEMPLATE\nprocessor = AutoProcessor.from_pretrained(model_id)\nprocessor.tokenizer = tokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:01.428368Z","iopub.execute_input":"2024-05-01T19:54:01.428682Z","iopub.status.idle":"2024-05-01T19:54:02.737941Z","shell.execute_reply.started":"2024-05-01T19:54:01.428634Z","shell.execute_reply":"2024-05-01T19:54:02.737064Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"990f9ff86e214011a7b262d673cd607b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bce8ee0cc04040b4bb4ad173dac7bd5d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9832ea64dbbb41ceb8110e7aa92a34e9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e7a10adb14b498fa4604e917470608e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1d77a9c87c7c40adac634521b4d7da14"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34b32d316d0940a09dd01cd3516e5232"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"code","source":"from PIL import Image\nimport requests\nimport numpy as np\nimport ast\nimport json\n\nclass LLavaDataCollator:\n    def __init__(self, processor, image_size=(640,480)):\n        self.processor = processor\n        self.image_size = image_size\n\n#     def resize_image(self, image_path):\n#         image = Image.open(image_path)\n#         image = image.resize(self.image_size, Image.ANTIALIAS)\n#         return image\n\n    def __call__(self, examples):\n        texts = []\n        images = []\n        for example in examples:\n#             messages = [{'content': [{'index': None, 'text': 'How many computer monitors are on the wooden desk?\\n', 'type': 'text'}, {'index': 0, 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are two computer monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'Can you describe the position of the chair and the person in relation to the wooden desk?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is sitting in the chair in front of the wooden desk, using a laptop computer. Their feet are up on the desk, and there are other computers in the background.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'What is the person doing with the laptop computer?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is using the laptop, possibly working or engaging in some recreational activity. They appear to be in a relaxed, lounging position with their feet up on the desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'How many computers can be seen in the image? Is there an additional computer aside from the laptop?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are a total of three computers visible in the image. These include the laptop being used by the person, and two additional monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': \"Considering the presence of the laptop computer and multiple monitors, what could be the possible implications for the person's work or personal preferences?\", 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': \"Given the presence of the laptop computer and multiple additional monitors, it is likely that the person either has a demanding job or tasks that require them to multitask across multiple screens, or they have a personal preference for an extended workspace to enhance their productivity or workflow.\\n\\nHaving multiple monitors can help improve efficiency and work quality in various ways, such as managing multiple open applications, web pages, and documents simultaneously without having to constantly switch back and forth between them on a single screen. People working in fields like programming, design, video editing, gaming, or finance might especially benefit from this setup, as it allows them to access and compare different sets of data or work on multiple tasks at the same time with ease.\\n\\nAdditionally, having a laptop computer in addition to the multiple monitors could indicate a need for mobility or flexibility in the person's work environment. They might travel frequently or work remotely, in which case a laptop would be essential. Having a laptop on hand also provides a convenient backup device should any technical issues arise with the other computers.\\n\\nAt a personal level, the person might simply enjoy the versatility and workspace expansion that comes with using a laptop alongside multiple monitors. For someone who spends a significant amount of time in front of screens, be it for work, communication, or leisure activities, such a setup can help create a comfortable and efficient workspace that caters to their needs and preferences.\", 'type': 'text'}], 'role': 'assistant'}]\n#             messages = {'content': [{'index': None, 'text': 'How many computer monitors are on the wooden desk?\\n', 'type': 'text'}, {'index': 0, 'text': None, 'type': 'image'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are two computer monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'Can you describe the position of the chair and the person in relation to the wooden desk?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is sitting in the chair in front of the wooden desk, using a laptop computer. Their feet are up on the desk, and there are other computers in the background.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'What is the person doing with the laptop computer?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'The person is using the laptop, possibly working or engaging in some recreational activity. They appear to be in a relaxed, lounging position with their feet up on the desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': 'How many computers can be seen in the image? Is there an additional computer aside from the laptop?', 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': 'There are a total of three computers visible in the image. These include the laptop being used by the person, and two additional monitors on the wooden desk.', 'type': 'text'}], 'role': 'assistant'}, {'content': [{'index': None, 'text': \"Considering the presence of the laptop computer and multiple monitors, what could be the possible implications for the person's work or personal preferences?\", 'type': 'text'}], 'role': 'user'}, {'content': [{'index': None, 'text': \"Given the presence of the laptop computer and multiple additional monitors, it is likely that the person either has a demanding job or tasks that require them to multitask across multiple screens, or they have a personal preference for an extended workspace to enhance their productivity or workflow.\\n\\nHaving multiple monitors can help improve efficiency and work quality in various ways, such as managing multiple open applications, web pages, and documents simultaneously without having to constantly switch back and forth between them on a single screen. People working in fields like programming, design, video editing, gaming, or finance might especially benefit from this setup, as it allows them to access and compare different sets of data or work on multiple tasks at the same time with ease.\\n\\nAdditionally, having a laptop computer in addition to the multiple monitors could indicate a need for mobility or flexibility in the person's work environment. They might travel frequently or work remotely, in which case a laptop would be essential. Having a laptop on hand also provides a convenient backup device should any technical issues arise with the other computers.\\n\\nAt a personal level, the person might simply enjoy the versatility and workspace expansion that comes with using a laptop alongside multiple monitors. For someone who spends a significant amount of time in front of screens, be it for work, communication, or leisure activities, such a setup can help create a comfortable and efficient workspace that caters to their needs and preferences.\", 'type': 'text'}], 'role': 'assistant'}]\n#             m = example[\"messages\"]\n#             d = json.loads(m)\n#             messages = [ { \"content\": [ { \"index\": null, \"text\": \"Who wrote this book?\\n\", \"type\": \"text\" }, { \"index\": 0, \"text\": null, \"type\": \"image\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Blue Star Coloring\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"What is the title of this book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Adult Coloring Book: Stress Relieving Animal Designs Volume 2\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"What is the genre of this book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Humor & Entertainment\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"Is this a comedy book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"Yes\", \"type\": \"text\" } ], \"role\": \"assistant\" }, { \"content\": [ { \"index\": null, \"text\": \"Is this a historical book?\", \"type\": \"text\" } ], \"role\": \"user\" }, { \"content\": [ { \"index\": null, \"text\": \"No\", \"type\": \"text\" } ], \"role\": \"assistant\" } ]\n#             messages.append(d)\n#             messages = [{\"content\": [{\"index\": null, \"text\": \"Who painted this portrait?\", \"type\": \"text\"}, {\"index\": 0, \"text\": null, \"type\": \"image\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"John Singleton Copley\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"When was this portrait created?\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"Between 1760 and 1770\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"Identify the technique used in the creation of this portrait.\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"Oil on canvas\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"What is the name of the subject featured in the portrait?\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"Nathaniel Hurd\", \"type\": \"text\"}], \"role\": \"assistant\"}, {\"content\": [{\"index\": null, \"text\": \"Describe the artistic style of this portrait.\", \"type\": \"text\"}], \"role\": \"user\"}, {\"content\": [{\"index\": null, \"text\": \"The style of the portrait is 18th-century American painting.\", \"type\": \"text\"}], \"role\": \"assistant\"}]\n            \n            messages = example[\"messages\"]\n#             print(type(messages))\n#             print(messages)\n            text = self.processor.tokenizer.apply_chat_template(\n                messages, tokenize=False, add_generation_prompt=False\n            )\n            texts.append(text)\n#             url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n#             image = Image.open(requests.get(url, stream=True).raw)\n#             print(example[\"images\"]) \n            image = Image.open(example[\"image\"])\n#             image = Image.open(\"/kaggle/input/images/images/1915.534_web.jpg\")\n#             resized_image = self.resize_image(example[\"images\"])\n#             print(example[\"images\"][0])\n#             images.append(example[\"images\"][0])\n#             print(image)\n            images.append(image)\n\n        batch = self.processor(texts, images, return_tensors=\"pt\", padding=True)\n#         print(batch)\n        labels = batch[\"input_ids\"].clone()\n#         print(labels)\n        if self.processor.tokenizer.pad_token_id is not None:\n            labels[labels == self.processor.tokenizer.pad_token_id] = -100\n        batch[\"labels\"] = labels\n#         print(batch)\n        return batch\n\n# Example usage:\n# processor = ...  # Initialize your processor\ndata_collator = LLavaDataCollator(processor)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:02.739637Z","iopub.execute_input":"2024-05-01T19:54:02.740065Z","iopub.status.idle":"2024-05-01T19:54:02.755973Z","shell.execute_reply.started":"2024-05-01T19:54:02.740033Z","shell.execute_reply":"2024-05-01T19:54:02.755003Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### **Importing Training and Test Dataset**","metadata":{}},{"cell_type":"code","source":"from datasets import load_dataset\ntrain_dataset_2 = load_dataset(\"/kaggle/input/train-6\")\ntest_dataset_2 = load_dataset(\"/kaggle/input/test-6\")","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:02.760166Z","iopub.execute_input":"2024-05-01T19:54:02.760610Z","iopub.status.idle":"2024-05-01T19:54:03.319302Z","shell.execute_reply.started":"2024-05-01T19:54:02.760585Z","shell.execute_reply":"2024-05-01T19:54:03.318359Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"14a999abd24148c7967aa717f47c22c6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6f04aaa2b3fc4b248da88699445cbbf6"}},"metadata":{}}]},{"cell_type":"code","source":"train_dataset_1 = train_dataset_2[\"train\"]\n# specific_train_dataset = train_dataset_2[\"train\"]\n# train_dataset_1 = specific_train_dataset.shuffle(seed=42).select(range(40))\ntest_dataset_1 = test_dataset_2[\"test\"]\nspecific_test_dataset = test_dataset_2[\"test\"]\neval_dataset_1 = specific_test_dataset.shuffle(seed=42).select(range(2))","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:03.320758Z","iopub.execute_input":"2024-05-01T19:54:03.321500Z","iopub.status.idle":"2024-05-01T19:54:03.340966Z","shell.execute_reply.started":"2024-05-01T19:54:03.321460Z","shell.execute_reply":"2024-05-01T19:54:03.339886Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir=\"llava-1.5-7b-hf-ft-mix-vsft-1\",\n    report_to=\"tensorboard\",\n    learning_rate=1.4e-5,\n    per_device_train_batch_size=2,\n    gradient_accumulation_steps=1,\n    logging_steps=5,\n    num_train_epochs=1,\n    push_to_hub=True,\n    gradient_checkpointing=True,\n    remove_unused_columns=False,\n    fp16=True,\n    bf16=False\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:03.342590Z","iopub.execute_input":"2024-05-01T19:54:03.343304Z","iopub.status.idle":"2024-05-01T19:54:03.380515Z","shell.execute_reply.started":"2024-05-01T19:54:03.343266Z","shell.execute_reply":"2024-05-01T19:54:03.379491Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"lora_config = LoraConfig(\n    r=64,\n    lora_alpha=16,\n    target_modules=\"all-linear\"\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:03.382126Z","iopub.execute_input":"2024-05-01T19:54:03.382478Z","iopub.status.idle":"2024-05-01T19:54:03.387780Z","shell.execute_reply.started":"2024-05-01T19:54:03.382452Z","shell.execute_reply":"2024-05-01T19:54:03.386634Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"%%capture\n!pip install huggingface_hub","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:03.389056Z","iopub.execute_input":"2024-05-01T19:54:03.389345Z","iopub.status.idle":"2024-05-01T19:54:17.410524Z","shell.execute_reply.started":"2024-05-01T19:54:03.389321Z","shell.execute_reply":"2024-05-01T19:54:17.409241Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:17.412115Z","iopub.execute_input":"2024-05-01T19:54:17.412453Z","iopub.status.idle":"2024-05-01T19:54:17.443680Z","shell.execute_reply.started":"2024-05-01T19:54:17.412421Z","shell.execute_reply":"2024-05-01T19:54:17.442661Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6eaa75bbf82d4edd92501396614cb10a"}},"metadata":{}}]},{"cell_type":"code","source":"trainer = SFTTrainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset_1,\n    eval_dataset=eval_dataset_1,\n    peft_config=lora_config,\n    dataset_text_field=\"text\",  # need a dummy field\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    dataset_kwargs={\"skip_prepare_dataset\": True},\n)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:31.740952Z","iopub.execute_input":"2024-05-01T19:54:31.741973Z","iopub.status.idle":"2024-05-01T19:54:35.080833Z","shell.execute_reply.started":"2024-05-01T19:54:31.741936Z","shell.execute_reply":"2024-05-01T19:54:35.079803Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:246: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Training**","metadata":{}},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:54:41.751892Z","iopub.execute_input":"2024-05-01T19:54:41.752303Z","iopub.status.idle":"2024-05-01T19:55:34.949881Z","shell.execute_reply.started":"2024-05-01T19:54:41.752270Z","shell.execute_reply":"2024-05-01T19:55:34.948798Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:37, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n  warnings.warn(\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=4, training_loss=3.6397864818573, metrics={'train_runtime': 52.6412, 'train_samples_per_second': 0.152, 'train_steps_per_second': 0.076, 'total_flos': 92569048289280.0, 'train_loss': 3.6397864818573, 'epoch': 1.0})"},"metadata":{}}]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### **Pushing to Hub**","metadata":{}},{"cell_type":"code","source":"trainer.push_to_hub()","metadata":{"execution":{"iopub.status.busy":"2024-05-01T19:55:51.395889Z","iopub.execute_input":"2024-05-01T19:55:51.397425Z","iopub.status.idle":"2024-05-01T19:56:32.568760Z","shell.execute_reply.started":"2024-05-01T19:55:51.397383Z","shell.execute_reply":"2024-05-01T19:56:32.567693Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:139: UserWarning: Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\n  warnings.warn(\"Setting `save_embedding_layers` to `True` as embedding layers found in `target_modules`.\")\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9027cdbc1db4a81a171af38e5978af4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/4.98k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"db8345265cd14d90a884019bf4402475"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"events.out.tfevents.1714593282.8a0e9ede4c8c.34.0:   0%|          | 0.00/5.86k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f38e702f866435a870e43b7087099d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81bdb2747284f3d802bb7eadb4b703e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73d5f297f6f47c5865a03af1a9651ce"}},"metadata":{}},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Shiv34/llava-1.5-7b-hf-ft-mix-vsft-1/commit/bce38c021da79a90a89f4e2f6733ba8b3d7cbb8b', commit_message='End of training', commit_description='', oid='bce38c021da79a90a89f4e2f6733ba8b3d7cbb8b', pr_url=None, pr_revision=None, pr_num=None)"},"metadata":{}}]},{"cell_type":"markdown","source":"### **Inferring from Hub**","metadata":{}},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\nfrom PIL import Image\nimport requests\n\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\n# model_id = \"HuggingFaceH4/vsft-llava-1.5-7b-hf\"\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n)\n\nbase_model = LlavaForConditionalGeneration.from_pretrained(model_id,\n                                                      quantization_config=quantization_config,\n                                                      torch_dtype=torch.float16)\n# Load the PEFT Lora model (adapter)\npeft_lora_adapter_path = \"Shiv34/llava-1.5-7b-hf-ft-mix-vsft-1\"\npeft_lora_adapter = PeftModel.from_pretrained(base_model, peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n\n# Merge the adapters into the base model\nbase_model.load_adapter(peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n\n# Now you can use the combined model (base model + PEFT Lora adapter) for inference\n# For example:\nprompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\nurl = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\n# processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\nprocessor = AutoProcessor.from_pretrained(\"HuggingFaceH4/vsft-llava-1.5-7b-hf-trl\")\ninputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n# ... process the image and create inputs ...\ngenerate_ids = base_model.generate(**inputs, max_new_tokens=15)\ndecoded_response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\nprint(\"Generated response:\", decoded_response)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T20:32:59.243467Z","iopub.status.idle":"2024-05-01T20:32:59.243816Z","shell.execute_reply.started":"2024-05-01T20:32:59.243649Z","shell.execute_reply":"2024-05-01T20:32:59.243664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### **Saving model locally**","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### **Inferring Locally**","metadata":{}},{"cell_type":"code","source":"%%capture\n!pip install -U \"transformers>=4.39.0\"\n!pip install peft bitsandbytes\n!pip install -U \"trl>=0.8.3\"\n!pip install deep_translator","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:55:12.372929Z","iopub.execute_input":"2024-05-01T21:55:12.373606Z","iopub.status.idle":"2024-05-01T21:56:18.371957Z","shell.execute_reply.started":"2024-05-01T21:55:12.373578Z","shell.execute_reply":"2024-05-01T21:56:18.370771Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoProcessor, TrainingArguments, LlavaForConditionalGeneration, BitsAndBytesConfig\nfrom trl import SFTTrainer\nfrom peft import LoraConfig","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:56:18.373954Z","iopub.execute_input":"2024-05-01T21:56:18.374240Z","iopub.status.idle":"2024-05-01T21:56:35.399353Z","shell.execute_reply.started":"2024-05-01T21:56:18.374211Z","shell.execute_reply":"2024-05-01T21:56:35.398204Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-05-01 21:56:24.926658: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-05-01 21:56:24.926754: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-05-01 21:56:25.052053: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from peft import PeftConfig, PeftModel\nfrom PIL import Image\nimport requests\n\n# model_id = \"llava-hf/llava-1.5-7b-hf\"\nmodel_id = \"llava-hf/llava-1.5-7b-hf\"\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n)\n\nbase_model = LlavaForConditionalGeneration.from_pretrained(model_id,\n                                                      quantization_config=quantization_config,\n                                                      torch_dtype=torch.float16)\n# Load the PEFT Lora model (adapter)\npeft_lora_adapter_path = \"Shiv34/llava-1.5-7b-hf-ft-mix-vsft-1\"\npeft_lora_adapter = PeftModel.from_pretrained(base_model, peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n\n# Merge the adapters into the base model\nbase_model.load_adapter(peft_lora_adapter_path, adapter_name=\"lora_adapter\")\n\nprocessor = AutoProcessor.from_pretrained(model_id)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:56:35.400808Z","iopub.execute_input":"2024-05-01T21:56:35.401754Z","iopub.status.idle":"2024-05-01T21:57:58.277558Z","shell.execute_reply.started":"2024-05-01T21:56:35.401721Z","shell.execute_reply":"2024-05-01T21:57:58.276806Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81b1901d2a914f5ea06997f738b0cf00"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/models/llava/configuration_llava.py:103: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n  warnings.warn(\n`low_cpu_mem_usage` was None, now set to True since model is quantized.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6383e459c70f407582c00d4d4572f792"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e30853f994cf4462858bee1ffdcfe958"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"be97aaff917448e4aea80a96258f2dce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a57cbafd93614dcfb535cbd0ece9ee34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f694fb3f27b44f19f0861eb15311ca9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42433f8c880c44f79a22d2427757a754"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dd206b53d794200b6da6c5e77a04c44"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/927 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6128818f7eb54c2e8a73be28eac76b46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/1.29G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b7c86e4b5e8f482192ccc38b551fb12e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json:   0%|          | 0.00/557 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b6651c664eb94129ac633426d297dfdc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c6f32cc2fbc4438b985fcf4e0192bc8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"59a499593c44422f8a3c3b9ac992ac99"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5ce8a35e84d14bde897bce1d1f28794a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"01e17101d915446fa4469487a0baa996"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/438 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23df12d6b28b48ca8d2f8cbd08bc1e36"}},"metadata":{}},{"name":"stderr","text":"Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### **Translating**","metadata":{}},{"cell_type":"code","source":"from deep_translator import GoogleTranslator\ndef deep_translator_ta_en(input_sentence):\n  english_translation = GoogleTranslator(source=\"ta\", target=\"en\").translate(input_sentence)\n  return english_translation\n\ndef deep_translator_en_ta(input_sentence):\n  bengali_translation = GoogleTranslator(source=\"en\", target=\"ta\").translate(input_sentence)\n  return bengali_translation\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:57:58.279612Z","iopub.execute_input":"2024-05-01T21:57:58.279926Z","iopub.status.idle":"2024-05-01T21:57:58.458263Z","shell.execute_reply.started":"2024-05-01T21:57:58.279900Z","shell.execute_reply":"2024-05-01T21:57:58.457498Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def inference(image_prompt, url):\n    prompt = f\"USER: <image>\\n{image_prompt} ASSISTANT:\"\n#     prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n    # url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n#     url = \"/kaggle/input/images/images/1921.428_web.jpg\"\n    image = Image.open(url)\n    #image = Image.open(requests.get(url, stream=True).raw)\n    # processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    # ... process the image and create inputs ...\n    generate_ids = base_model.generate(**inputs, max_new_tokens=15)\n    decoded_response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n#     print(\"Generated response:\", decoded_response)\n    return decoded_response\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:57:58.459372Z","iopub.execute_input":"2024-05-01T21:57:58.459988Z","iopub.status.idle":"2024-05-01T21:57:58.466059Z","shell.execute_reply.started":"2024-05-01T21:57:58.459961Z","shell.execute_reply":"2024-05-01T21:57:58.465126Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"url = input(\"à®ªà®Ÿà®¤à¯à®¤à®¿à®©à¯ url à® à®‰à®³à¯à®³à®¿à®Ÿà®µà¯à®®à¯: \")\ninput_sentence = input(\"à¦›à¦¬à¦¿ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦²à¦¿à¦–à§à¦¨: \")\nimage_prompt = deep_translator_ta_en(input_sentence)\nresponse = inference(image_prompt, url)\nassistant_index = response.find(\"ASSISTANT:\")\nextracted_string = response[assistant_index + len(\"ASSISTANT:\"):].strip()\noutput = deep_translator_en_ta(extracted_string)\nprint(\"à¦¬à¦Ÿà§€: \", output)","metadata":{"execution":{"iopub.status.busy":"2024-05-01T21:57:58.467320Z","iopub.execute_input":"2024-05-01T21:57:58.467682Z","iopub.status.idle":"2024-05-01T22:00:04.162816Z","shell.execute_reply.started":"2024-05-01T21:57:58.467650Z","shell.execute_reply":"2024-05-01T22:00:04.161863Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdin","text":"à®ªà®Ÿà®¤à¯à®¤à®¿à®©à¯ url à® à®‰à®³à¯à®³à®¿à®Ÿà®µà¯à®®à¯:  /kaggle/input/images1/Images/9_462f776b.jpg\nà¦›à¦¬à¦¿ à¦¸à¦®à§à¦ªà¦°à§à¦•à§‡ à¦†à¦ªà¦¨à¦¾à¦° à¦ªà§à¦°à¦¶à§à¦¨ à¦²à¦¿à¦–à§à¦¨:  à®ªà®Ÿà®¤à¯à®¤à¯ˆ à®µà®¿à®µà®°à®¿à®•à¯à®•à®µà¯à®®à¯\n"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"à¦¬à¦Ÿà§€:  à®ªà®Ÿà®¤à¯à®¤à®¿à®²à¯ à®ªà®²à¯à®µà¯‡à®±à¯ à®µà®¿à®²à®™à¯à®•à¯à®•à®³à¯ à®®à®±à¯à®±à¯à®®à¯ à®®à®©à®¿à®¤ à®‰à®°à¯à®µà®™à¯à®•à®³à¯ à®ªà¯‹à®©à¯à®± à®µà¯†à®³à¯à®³à®¿à®¤à¯ à®¤à®•à®Ÿà¯ à®‰à®³à¯à®³à®¤à¯\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install gradio","metadata":{"execution":{"iopub.status.busy":"2024-05-01T22:00:04.163887Z","iopub.execute_input":"2024-05-01T22:00:04.164173Z","iopub.status.idle":"2024-05-01T22:00:24.122699Z","shell.execute_reply.started":"2024-05-01T22:00:04.164148Z","shell.execute_reply":"2024-05-01T22:00:24.121612Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Collecting gradio\n  Downloading gradio-4.28.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: aiofiles<24.0,>=22.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (22.1.0)\nRequirement already satisfied: altair<6.0,>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (5.3.0)\nRequirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from gradio) (0.108.0)\nCollecting ffmpy (from gradio)\n  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting gradio-client==0.16.0 (from gradio)\n  Downloading gradio_client-0.16.0-py3-none-any.whl.metadata (7.1 kB)\nRequirement already satisfied: httpx>=0.24.1 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.27.0)\nRequirement already satisfied: huggingface-hub>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.22.2)\nRequirement already satisfied: importlib-resources<7.0,>=1.3 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.1.1)\nRequirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.1.2)\nRequirement already satisfied: markupsafe~=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.3)\nRequirement already satisfied: matplotlib~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.7.5)\nRequirement already satisfied: numpy~=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (3.9.10)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from gradio) (21.3)\nRequirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.1.4)\nRequirement already satisfied: pillow<11.0,>=8.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (9.5.0)\nRequirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (2.5.3)\nRequirement already satisfied: pydub in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.1)\nCollecting python-multipart>=0.0.9 (from gradio)\n  Downloading python_multipart-0.0.9-py3-none-any.whl.metadata (2.5 kB)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (6.0.1)\nCollecting ruff>=0.2.2 (from gradio)\n  Downloading ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\nCollecting semantic-version~=2.0 (from gradio)\n  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\nCollecting tomlkit==0.12.0 (from gradio)\n  Downloading tomlkit-0.12.0-py3-none-any.whl.metadata (2.7 kB)\nCollecting typer<1.0,>=0.12 (from gradio)\n  Downloading typer-0.12.3-py3-none-any.whl.metadata (15 kB)\nRequirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (4.9.0)\nCollecting urllib3~=2.0 (from gradio)\n  Downloading urllib3-2.2.1-py3-none-any.whl.metadata (6.4 kB)\nRequirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from gradio) (0.25.0)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from gradio-client==0.16.0->gradio) (2024.2.0)\nCollecting websockets<12.0,>=10.0 (from gradio-client==0.16.0->gradio)\n  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: jsonschema>=3.0 in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (4.20.0)\nRequirement already satisfied: toolz in /opt/conda/lib/python3.10/site-packages (from altair<6.0,>=4.2.0->gradio) (0.12.1)\nRequirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (4.2.0)\nRequirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (2024.2.2)\nRequirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.0.5)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (3.6)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from httpx>=0.24.1->gradio) (1.3.0)\nRequirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\nRequirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.2.0)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (4.47.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (1.4.5)\nRequirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (3.1.1)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.10/site-packages (from matplotlib~=3.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas<3.0,>=1.0->gradio) (2023.4)\nRequirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (0.6.0)\nRequirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->gradio) (2.14.6)\nRequirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\nRequirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.10/site-packages (from typer<1.0,>=0.12->gradio) (13.7.0)\nRequirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->gradio) (0.32.0.post1)\nRequirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\nRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\nRequirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\nRequirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.16.2)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.10/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.17.2)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx>=0.24.1->gradio) (1.2.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\nRequirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\nDownloading gradio-4.28.3-py3-none-any.whl (12.2 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m44.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n\u001b[?25hDownloading gradio_client-0.16.0-py3-none-any.whl (314 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m314.4/314.4 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.12.0-py3-none-any.whl (37 kB)\nDownloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\nDownloading ruff-0.4.2-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.8 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.8/8.8 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\nDownloading typer-0.12.3-py3-none-any.whl (47 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading urllib3-2.2.1-py3-none-any.whl (121 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: ffmpy\n  Building wheel for ffmpy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=a82525f3b7142e639077c6e7e7b2881bc8f6cc2bece3ccc3f1d4698f00521711\n  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\nSuccessfully built ffmpy\nInstalling collected packages: ffmpy, websockets, urllib3, tomlkit, semantic-version, ruff, python-multipart, typer, gradio-client, gradio\n  Attempting uninstall: websockets\n    Found existing installation: websockets 12.0\n    Uninstalling websockets-12.0:\n      Successfully uninstalled websockets-12.0\n  Attempting uninstall: urllib3\n    Found existing installation: urllib3 1.26.18\n    Uninstalling urllib3-1.26.18:\n      Successfully uninstalled urllib3-1.26.18\n  Attempting uninstall: tomlkit\n    Found existing installation: tomlkit 0.12.4\n    Uninstalling tomlkit-0.12.4:\n      Successfully uninstalled tomlkit-0.12.4\n  Attempting uninstall: typer\n    Found existing installation: typer 0.9.0\n    Uninstalling typer-0.9.0:\n      Successfully uninstalled typer-0.9.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ntensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\ndistributed 2023.7.1 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nkfp 2.5.0 requires google-cloud-storage<3,>=2.2.1, but you have google-cloud-storage 1.44.0 which is incompatible.\nkfp 2.5.0 requires urllib3<2.0.0, but you have urllib3 2.2.1 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.4.1 which is incompatible.\nspacy 3.7.3 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\ntensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.2.1 which is incompatible.\nweasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\nydata-profiling 4.6.4 requires numpy<1.26,>=1.16.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed ffmpy-0.3.2 gradio-4.28.3 gradio-client-0.16.0 python-multipart-0.0.9 ruff-0.4.2 semantic-version-2.10.0 tomlkit-0.12.0 typer-0.12.3 urllib3-2.1.0 websockets-11.0.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"pip install deep_translator","metadata":{"execution":{"iopub.status.busy":"2024-05-01T22:00:24.124004Z","iopub.execute_input":"2024-05-01T22:00:24.124294Z","iopub.status.idle":"2024-05-01T22:00:36.191816Z","shell.execute_reply.started":"2024-05-01T22:00:24.124264Z","shell.execute_reply":"2024-05-01T22:00:36.190553Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: deep_translator in /opt/conda/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /opt/conda/lib/python3.10/site-packages (from deep_translator) (4.12.2)\nRequirement already satisfied: requests<3.0.0,>=2.23.0 in /opt/conda/lib/python3.10/site-packages (from deep_translator) (2.31.0)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep_translator) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0,>=2.23.0->deep_translator) (2024.2.2)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import gradio as gr\nfrom deep_translator import GoogleTranslator  # Switch to GoogleTranslator for reliability\n\ndef inference(image, image_prompt):\n    \"\"\"Combines user prompt and ASSISTANT prefix for model input\"\"\"\n    prompt = f\"USER: <image>\\n{image_prompt} ASSISTANT:\"\n\n    # Assuming your model can handle PIL images (adjust if needed)\n    image = image.convert(\"RGB\")\n\n    # Assuming your model has a `processor` attribute and uses PyTorch tensors\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    try:\n        generate_ids = base_model.generate(**inputs, max_new_tokens=15)\n    except Exception as e:  # Catch potential model errors\n        return f\"Error: {e}\"\n\n    decoded_response = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n    return decoded_response\n\ndef main(image, input_sentence):\n    \"\"\"Translates input, calls inference, extracts assistant response, and translates back\"\"\"\n    translated_prompt = GoogleTranslator(source='ta', target='en').translate(input_sentence)  # Tamil to English\n    response = inference(image, translated_prompt)\n    if \"ASSISTANT:\" not in response:  # Handle cases where model doesn't return ASSISTANT marker\n        return \"The model couldn't assist with this image.\"\n\n    assistant_index = response.find(\"ASSISTANT:\")\n    extracted_string = response[assistant_index + len(\"ASSISTANT:\"):].strip()\n    translated_output = GoogleTranslator(source='en', target='ta').translate(extracted_string)  # English to Tamil\n    return translated_output\n\n# Interface with image upload and improved error handling\niface = gr.Interface(\n    fn=main,\n    inputs=[gr.Image(type=\"pil\"), \"text\"],\n    outputs=\"text\",\n    title=\"Image Assistant\",\n    description=\"Translate Tamil input, get translated image content in Tamil.\",\n)\n\niface.launch()\n","metadata":{"execution":{"iopub.status.busy":"2024-05-01T22:02:14.642873Z","iopub.execute_input":"2024-05-01T22:02:14.643218Z","iopub.status.idle":"2024-05-01T22:02:14.986410Z","shell.execute_reply.started":"2024-05-01T22:02:14.643194Z","shell.execute_reply":"2024-05-01T22:02:14.985086Z"},"trusted":true},"execution_count":1,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgradio\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mgr\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdeep_translator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GoogleTranslator  \u001b[38;5;66;03m# Assuming deep_translator is for translation\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function for image captioning (replace with your actual inference function)\u001b[39;00m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'gradio'"],"ename":"ModuleNotFoundError","evalue":"No module named 'gradio'","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"jupyter":{"outputs_hidden":true},"collapsed":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}